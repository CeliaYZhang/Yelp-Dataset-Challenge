{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "nbpresent": {
     "id": "946947ae-8c8a-4596-8edf-737b0db60fe6"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "\n",
    "import imageio\n",
    "import loading\n",
    "import tqdm\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn import svm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "nbpresent": {
     "id": "8cd62661-4480-4c87-9a52-6224dc66d451"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"DEFINE CONSTANTS HERE\"\"\"\n",
    "\n",
    "DATA_PATH           = '../data'\n",
    "VISUALIZATION_PATH  = '../visualization'\n",
    "\n",
    "MISSING_VALUE       = '<NONE>'       # Used for the 'neighborhood' and 'city' attributes.\n",
    "DEFAULT_NEI_P       = 0.2            # Default percentile of neighborhoods to keep.\n",
    "DEFAULT_CITY_P      = 0.1            # Default percentile of cities to keep.\n",
    "DEFAULT_ATT_P       = 1.0            # Default percentile of attributes to keep.\n",
    "DEFAULT_CAT_P       = 0.5            # Default percentile of categories to keep.\n",
    "DEFAULT_HRS_P       = 1.0            # Default percentile of hours to keep.\n",
    "\n",
    "TIME_GRANULARITY    = 1              # Granularity (ticks/hr) of time calculations, factor of 60. \n",
    "\n",
    "SLICE_BY            = ['Restaurants']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "nbpresent": {
     "id": "291b4ee3-21ef-443a-8b73-4c02ba5b46cb"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Loads the json file of the given dataset name.\"\"\"\n",
    "def load(name):\n",
    "  start = time.time()\n",
    "  data = loading.read_df_from_json('%s/%s.json' % (DATA_PATH, name))\n",
    "  print 'time to load \\'%s\\': %.3fs' % (name, time.time() - start)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "nbpresent": {
     "id": "abd975c5-2824-44cd-87ad-9556d53f7c99"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Cleans the business dataset.\"\"\"\n",
    "def clean_business(business):\n",
    "  print 'Replacing %s with %s.' % (u'Montréal', u'Montreal')\n",
    "  business['city'].replace(to_replace=u'Montréal', \n",
    "                           value=u'Montreal',\n",
    "                           inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "nbpresent": {
     "id": "04ac22ac-3fa4-423a-bc85-82070c57e991"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Plots all businesses on the world map for visualization purposes.\"\"\"\n",
    "def plot_business(business):\n",
    "  points = business[['latitude', 'longitude']]\n",
    "\n",
    "  img = imageio.imread(VISUALIZATION_PATH + '/raw_map.jpg').astype('int64')\n",
    "  img = img / 4               # Dim map.\n",
    "  img = img[8:-8,8:-8,:]      # Clip borders.\n",
    "  H, W, _ = img.shape\n",
    "  \n",
    "  scalar = 10                 # Amount to add to each channel.\n",
    "  delta = np.zeros((H, W), dtype='int64')\n",
    "  \n",
    "  def get_xy(latitude, longitude):\n",
    "    x = (W - 1) * (180.0 + longitude) / 360.0\n",
    "    y = (H - 1) * (90.0 - latitude) / 180.0\n",
    "    return int(x), int(y)\n",
    "\n",
    "  for row in points.itertuples():\n",
    "    latitude, longitude = row.latitude, row.longitude\n",
    "    if not math.isnan(latitude) and not math.isnan(longitude):\n",
    "      x, y = get_xy(latitude, longitude)\n",
    "      delta[y,x] += scalar\n",
    "\n",
    "  img += np.expand_dims(delta, axis=-1).repeat(3, axis=-1)\n",
    "  img = img.clip(0, 255).astype('uint8')\n",
    "  \n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "nbpresent": {
     "id": "eec66196-2d62-43f9-83c7-bf6a4fb555b3"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Returns the count_dict as a sorted list.\"\"\"\n",
    "def to_list(count_dict):\n",
    "  return sorted([(k, count_dict[k]) for k in count_dict], key=lambda v: v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "nbpresent": {
     "id": "1ef7dedb-efaa-49d1-8edc-6150727a921f"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Converts a dict of counts (key, int) into a list of top features.\n",
    "\n",
    "   Takes either top N (int) features, or top PERCENTILE (float) by occurrence.\n",
    "\n",
    "   Example usage:\n",
    "     top_features(count_dict, 0.1)                  # Returns top 10% of elements.\n",
    "     top_features(count_dict, top_n=5)              # Returns top 5 elements.\n",
    "\"\"\"\n",
    "def top_features(count_dict, percentile=None, n=None, verbose=True):\n",
    "  if n is None:\n",
    "    if percentile is None:\n",
    "      raise Exception\n",
    "    n = int(percentile * len(count_dict))\n",
    "\n",
    "  l = heapq.nlargest(n, count_dict, key=lambda k: count_dict[k])\n",
    "  \n",
    "  if verbose:\n",
    "    percentage = 0.0 if not len(count_dict) else 100.0 * n / len(count_dict)\n",
    "    params = (n, len(count_dict), percentage, 0 if not l else count_dict[l[-1]])\n",
    "    print 'Took %d elements out of %d (%2.1f%%). Cutoff was >= %d.' % params\n",
    "    \n",
    "  return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "nbpresent": {
     "id": "653df377-dfa1-4b2c-8656-280f84949f2d"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Methods for converting a tuple from itertuples() to a feature list.\"\"\"\n",
    "\n",
    "# Return neighborhood concatenated with city, or MISSING_VALUE if empty. \n",
    "def get_neighborhood(tup):\n",
    "  assert type(tup.neighborhood) is unicode\n",
    "  return [tup.neighborhood + '/' + tup.city if tup.neighborhood else MISSING_VALUE]\n",
    "\n",
    "# Return city, or MISSING_VALUE if empty.\n",
    "def get_city(tup):\n",
    "  assert type(tup.city) is unicode\n",
    "  return [tup.city if tup.city else MISSING_VALUE]\n",
    "\n",
    "# Recursively process attributes dict to get indicators for all attributes.\n",
    "def get_attributes(tup):\n",
    "  def _recurse(attributes, prefix):\n",
    "    assert type(attributes) is dict\n",
    "    l = []\n",
    "    for k, v in attributes.items():\n",
    "      if type(v) is bool:\n",
    "        l.append(prefix + '/' + k)\n",
    "      elif type(v) is unicode:\n",
    "        l.append(prefix + '/' + k + '/' + v)\n",
    "      elif type(v) is int:\n",
    "        l.append(prefix + '/' + k + '/' + str(v))\n",
    "      elif type(v) is dict:\n",
    "        l += _recurse(attributes[k], prefix=k)\n",
    "      else:\n",
    "        assert False  # Invalid type in attributes.\n",
    "    return l\n",
    "  return _recurse(tup.attributes, prefix='')\n",
    "\n",
    "# Return categories.\n",
    "def get_categories(tup):\n",
    "  assert type(tup.categories) is list\n",
    "  return tup.categories\n",
    "\n",
    "\"\"\"Helper methods for get_hours(), which determines which ticks of time the business is open.\n",
    "   Each tick of time corresponds to an index in [0, _max_ticks()).\n",
    "\n",
    "   In _time_to_dt_index, ROUND_UP determines what happens when the time falls\n",
    "   in between ticks. By default, the time is rounded up.\n",
    "\"\"\"\n",
    "# An ordering of the days of the week, and a map from str --> index\n",
    "_day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "_day_index = {d:i for i, d in enumerate(_day_order)}\n",
    "\n",
    "# Maximum number of ticks\n",
    "def _max_ticks():\n",
    "  return 7 * 24 * TIME_GRANULARITY\n",
    "\n",
    "# Given day of the week and time, returns the corresponding tick in [0, _max_ticks()).\n",
    "def _time_to_dt_index(day, time, round_up=True):\n",
    "  [hour, minutes] = time.split(':')\n",
    "  hour_index = int(hour) * TIME_GRANULARITY\n",
    "  min_index  = int(minutes) / (60 / TIME_GRANULARITY)\n",
    "  if round_up and int(minutes) % (60 / TIME_GRANULARITY) > 0:\n",
    "    min_index += 1\n",
    "  return _day_index[day] * 24 * TIME_GRANULARITY + hour_index + min_index\n",
    "\n",
    "# Cache the string corresponding to each tick of time.\n",
    "_timestr_cache = ['%s/%02d:%02d' % (_day_order[day], hour, min_index * 60 / TIME_GRANULARITY)\n",
    "                  for day in range(0, 7)\n",
    "                  for hour in range(0, 24)\n",
    "                  for min_index in range(0, TIME_GRANULARITY)]\n",
    "\n",
    "def get_hours(tup):\n",
    "  assert type(tup.hours) is dict\n",
    "  l = []\n",
    "  for day, hours in tup.hours.items():\n",
    "    open_time, close_time = hours.split('-')\n",
    "    open_index = _time_to_dt_index(day, open_time)\n",
    "    close_index = _time_to_dt_index(day, close_time)\n",
    "\n",
    "    assert 0 <= open_index and close_index <= _max_ticks()\n",
    "    \n",
    "    # Handle the case where close_index is for the following day.\n",
    "    if close_index <= open_index:\n",
    "      close_index += 24 * TIME_GRANULARITY\n",
    "    \n",
    "    # Append the slice of the _time_str_cache, handling wrap-around appropriately.\n",
    "    l += _timestr_cache[open_index:min(close_index, _max_ticks())]\n",
    "    if close_index > _max_ticks():\n",
    "      l += _timestr_cache[0:close_index - _max_ticks()]\n",
    "      \n",
    "  return l\n",
    "\n",
    "# Function to retrieve all features of a given row tuple.\n",
    "all_fns = [get_neighborhood, get_city, get_attributes, get_categories, get_hours]\n",
    "def get_all_features(tup):\n",
    "  all_features = set([])\n",
    "  for fn in all_fns:\n",
    "    all_features |= set(fn(tup))\n",
    "  return all_features\n",
    "\n",
    "# Returns the value to regress on for the row tuple.\n",
    "def get_target(tup):\n",
    "  return float(tup.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "nbpresent": {
     "id": "e0be7c98-dad3-447c-97a2-d6c186d78da5"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Get the features that we will use for \n",
    "   neighborhoods, cities, attributes, categories.\n",
    "\n",
    "   By default, take:\n",
    "     TOP 20% OF neighborhoods\n",
    "     TOP 10% OF cities\n",
    "         ALL OF attributes\n",
    "     TOP 50% OF categories\n",
    "         ALL OF hours\n",
    "   \n",
    "   Returns a length 6 tuple:\n",
    "     (nei_set, city_set, att_set, cat_set, hours_set, debug_vals)\n",
    "\"\"\"\n",
    "def get_feature_sets(business, slice_by=[],\n",
    "                     percentiles=[DEFAULT_NEI_P, DEFAULT_CITY_P, DEFAULT_ATT_P, \n",
    "                                  DEFAULT_CAT_P, DEFAULT_HRS_P]):\n",
    "  def has_required_features(all_features):\n",
    "    all_features_set = set([])\n",
    "    for features in all_features:\n",
    "      all_features_set |= set(features)\n",
    "    for f in slice_by:\n",
    "      if f not in all_features_set:\n",
    "        return False\n",
    "    return True\n",
    "  \n",
    "  fn_counts_percentile = zip(all_fns, [defaultdict(int) for _ in range(len(all_fns))], percentiles)\n",
    "\n",
    "  for tup in tqdm.tqdm(business.itertuples()):\n",
    "    all_features = [fn(tup) for fn, _, _ in fn_counts_percentile]\n",
    "    if not has_required_features(all_features):\n",
    "      continue\n",
    "    for features, (_, counts, _) in zip(all_features, fn_counts_percentile):\n",
    "      for f in features:\n",
    "        counts[f] += 1\n",
    "\n",
    "  debug_val = [(fn.__name__, to_list(counts)) for fn, counts, _ in fn_counts_percentile]\n",
    "  \n",
    "  return [top_features(counts, percentile) for fn, counts, percentile in fn_counts_percentile], debug_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "nbpresent": {
     "id": "289f6d77-a27a-4cf4-8559-edf3f068896d"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Gets a mapping from feature name to feature index and vice versa.\"\"\"\n",
    "def get_feature_maps(feature_sets, start_index=0):  \n",
    "  # Assert that there are no overlapping names.\n",
    "  union = set([])\n",
    "  for s in feature_sets:\n",
    "    union |= set(s)\n",
    "  assert len(union) == sum([len(s) for s in feature_sets])\n",
    "\n",
    "  name_to_index, index_to_name = {}, {}\n",
    "  for i, feature in enumerate(union):\n",
    "    name_to_index[feature] = start_index + i\n",
    "    index_to_name[start_index + i] = feature\n",
    "\n",
    "  return name_to_index, index_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_by_feature(x, y, name_to_index, feature_name):\n",
    "  if feature_name not in name_to_index:\n",
    "    print 'Feature %s not found.' % feature_name\n",
    "    return x, y\n",
    "  feature_index = name_to_index[feature_name]\n",
    "  indices = np.where(x[:,feature_index] == 1)\n",
    "  return x[indices], y[indices]\n",
    "\n",
    "def slice_by_features(x, y, name_to_index, slice_by):\n",
    "  for feature_name in slice_by:\n",
    "    x, y = slice_by_feature(x, y, name_to_index, feature_name)\n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "nbpresent": {
     "id": "ec08aa75-b5c0-495a-92a0-7cfea7e776ce"
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Creates a list of data points for multivariate linear regression.\"\"\"\n",
    "def get_training_data(business, name_to_index, slice_by=[]):\n",
    "  x = np.zeros((business.shape[0], len(name_to_index)), dtype='float32')\n",
    "  y = np.zeros(business.shape[0], dtype='float32')\n",
    "  \n",
    "  for i, tup in tqdm.tqdm(enumerate(business.itertuples())):\n",
    "    all_features = get_all_features(tup)\n",
    "    for f in all_features:\n",
    "      if f in name_to_index:\n",
    "        x[i,name_to_index[f]] = 1\n",
    "    y[i] = get_target(tup)\n",
    "  \n",
    "  return slice_by_features(x, y, name_to_index, slice_by)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "nbpresent": {
     "id": "50857f70-79cf-475d-9c4e-739505c9f2ab"
    }
   },
   "source": [
    "Run business regression.\n",
    "\n",
    "NOT USABLE:\n",
    "- business_id\n",
    "- name\n",
    "\n",
    "PROBABLY NOT USABLE:\n",
    "- latitude\n",
    "- longitude\n",
    "- postal code\n",
    "- address\n",
    "- is_open\n",
    "- review_count\n",
    "- state\n",
    "\n",
    "REGRESS ON:\n",
    "- stars  (1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5)\n",
    "\n",
    "FEATURES:\n",
    "- neighborhood/city  --> indicators (~20%)\n",
    "- city               --> indicators (~10%)\n",
    "- attributes         --> indicators (process types differently, each has a separate indicator)\n",
    "- categories         --> indicators (~50%)\n",
    "- hours              --> indicators for every hour/half hour/quarter of the hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "nbpresent": {
     "id": "284f8122-8e46-413c-9d39-f07431deb2e8"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to load 'business': 20.628s\n",
      "Replacing Montréal with Montreal.\n"
     ]
    }
   ],
   "source": [
    "business = load('business')b\n",
    "clean_business(business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "nbpresent": {
     "id": "3af4073c-1b36-4790-bc22-9e549e7f02a7"
    }
   },
   "outputs": [],
   "source": [
    "# img = plot_business(business)\n",
    "# plt.imshow(img)\n",
    "# plt.show()\n",
    "# imageio.imsave(VISUALIZATION_PATH + '/business_map.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "nbpresent": {
     "id": "acd359bb-dccd-42b8-b6ff-d6fa3eb5c008"
    },
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 0 elements out of 550 (0.0%). Cutoff was >= 0.\n",
      "Took 0 elements out of 734 (0.0%). Cutoff was >= 0.\n",
      "Took 70 elements out of 100 (70.0%). Cutoff was >= 752.\n",
      "Took 97 elements out of 651 (14.9%). Cutoff was >= 175.\n",
      "Took 0 elements out of 168 (0.0%). Cutoff was >= 0.\n"
     ]
    }
   ],
   "source": [
    "SLICE_BY = ['Restaurants']\n",
    "\n",
    "feature_sets, debug_val = get_feature_sets(business, slice_by=SLICE_BY, \n",
    "                                           percentiles=[0, 0, 0.7, 0.15, 0.0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "nbpresent": {
     "id": "3c317a23-36fd-4efa-8cdb-920fe50be73b"
    }
   },
   "outputs": [],
   "source": [
    "feature_maps = get_feature_maps(feature_sets)\n",
    "name_to_index, index_to_name = feature_maps  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "nbpresent": {
     "id": "402fdf70-91e4-4be3-b179-262296e16195"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          \r"
     ]
    }
   ],
   "source": [
    "x, y = get_training_data(business, name_to_index, slice_by=SLICE_BY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing by: ['Restaurants']\n",
      "Samples in slice: 51613\n",
      "\n",
      "Intercept = 3.36518456985\n",
      "\n",
      "Food Trucks 0.453103035284\n",
      "Vegan 0.307068254562\n",
      "Latin American 0.306034198123\n",
      "Caribbean 0.296301275254\n",
      "French 0.27946296579\n",
      "Hotels & Travel 0.260133183633\n",
      "Modern European 0.251097020068\n",
      "Food Stands 0.250909546574\n",
      "Specialty Food 0.231032568397\n",
      "German 0.223879627584\n",
      "Swabian 0.221521238943\n",
      "Chicken Shop 0.220045399246\n",
      "Fish & Chips 0.217996759451\n",
      "Portuguese 0.214262340238\n",
      "Hawaiian 0.212582334901\n",
      "Dive Bars 0.195085614846\n",
      "Hot Dogs 0.174344622817\n",
      "Cafes 0.171148198841\n",
      "Soul Food 0.159251981817\n",
      "/BusinessAcceptsBitcoin 0.154675084625\n",
      "...\n",
      "/RestaurantsAttire/casual -0.073986050238\n",
      "Imported Food -0.0841129490088\n",
      "American (Traditional) -0.0880086412048\n",
      "Pizza -0.0886453161762\n",
      "Venues & Event Spaces -0.124112273937\n",
      "/BusinessAcceptsCreditCards -0.125439466803\n",
      "Sports Bars -0.127505819715\n",
      "/NoiseLevel/loud -0.128328313941\n",
      "Chinese -0.130034773375\n",
      "Ethnic Food -0.130089626883\n",
      "Pakistani -0.163330543992\n",
      "Burgers -0.243899187156\n",
      "Tex-Mex -0.296597369096\n",
      "Chicken Wings -0.303362811971\n",
      "BusinessParking/validated -0.336053200353\n",
      "/NoiseLevel/very_loud -0.344524152059\n",
      "Bagels -0.348587433865\n",
      "Hotels -0.399591040999\n",
      "Buffets -0.402746901662\n",
      "Fast Food -0.536001668883\n",
      "\n",
      "0.703477912694\n"
     ]
    }
   ],
   "source": [
    "print 'Slicing by:', SLICE_BY\n",
    "print 'Samples in slice:', len(x)\n",
    "print\n",
    "\n",
    "reg = linear_model.Ridge(alpha=0.5)\n",
    "reg.fit(x, y)\n",
    "\n",
    "print 'Intercept =', reg.intercept_\n",
    "print\n",
    "\n",
    "N = 20\n",
    "coefs = [(index_to_name[i], c) for i, c in enumerate(reg.coef_)]\n",
    "for n, c in heapq.nlargest(N, coefs, key=lambda v: v[1]):\n",
    "  print n, c\n",
    "print '...'\n",
    "for n, c in reversed(heapq.nsmallest(N, coefs, key=lambda v: v[1])):\n",
    "  print n, c\n",
    "print\n",
    "\n",
    "y_hat = reg.predict(x)\n",
    "print np.sqrt(mean_squared_error(y, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splits list of data points 80/20 for training versus test.\"\"\"\n",
    "def training_test_split(x, y):\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    cutoff = int(.8 * x.shape[0])\n",
    "    training_idx, test_idx = indices[:cutoff], indices[cutoff+1:]\n",
    "    x_train, x_test = x[training_idx,:], x[test_idx,:]\n",
    "    y_train, y_test = y[training_idx], y[test_idx]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing by: ['Restaurants']\n",
      "Samples in training set: 41290\n",
      "\n",
      "test MSE 0.70817920643\n",
      "\n",
      "Slicing by: ['Restaurants']\n",
      "Samples in slice: 41290\n",
      "\n",
      "means test MSE 0.785963\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = training_test_split(x, y)\n",
    "print 'Slicing by:', SLICE_BY\n",
    "print 'Samples in training set:', len(x_train)\n",
    "print\n",
    "reg = linear_model.Ridge(alpha=0.7)\n",
    "reg.fit(x_train, y_train)\n",
    "y_hat = reg.predict(x_test)\n",
    "print \"test MSE\", np.sqrt(mean_squared_error(y_test, y_hat))\n",
    "print\n",
    "\n",
    "print 'Slicing by:', SLICE_BY\n",
    "print 'Samples in slice:', len(x_train)\n",
    "print\n",
    "\n",
    "y_mean = np.mean(y_train)\n",
    "print \"means test MSE\", np.sqrt(mean_squared_error(np.repeat(y_mean, len(y_test)), y_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def index_submaps(name_array):\n",
    "    sub_name_to_ind = {}\n",
    "    sub_ind_to_name = {}\n",
    "    ind = 0\n",
    "    for n in name_array:\n",
    "        sub_name_to_ind[n] = ind\n",
    "        sub_ind_to_name[ind] = n\n",
    "        ind += 1\n",
    "    return sub_name_to_ind, sub_ind_to_name\n",
    "\n",
    "cultural_cols = ['Latin American', 'Caribbean', 'Modern European', 'German', 'Portuguese', 'Hawaiian', \n",
    "                'American (Traditional)', 'Chinese', 'Pakistani', 'Tex-Mex']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Feature Restaurants not found.            \n"
     ]
    }
   ],
   "source": [
    "sub_cult_to_ind, sub_ind_to_cult = index_submaps(cultural_cols)\n",
    "x_cult, y_cult = get_training_data(business, sub_cult_to_ind, slice_by=SLICE_BY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Slicing by: ['Restaurants']\n",
      "Samples in slice: 156639\n",
      "\n",
      "Intercept = 3.66857220381\n",
      "\n",
      "Modern European 0.300015062522\n",
      "Latin American 0.217323481932\n",
      "Caribbean 0.169281552524\n",
      "Hawaiian 0.166152837305\n",
      "German 0.145412203815\n",
      "Portuguese 0.0523951887081\n",
      "Pakistani -0.186747522438\n",
      "American (Traditional) -0.336699668781\n",
      "Chinese -0.346919122557\n",
      "Tex-Mex -0.66409477007\n",
      "\n",
      "MSE cultural 0.972964587125\n",
      "\n",
      "Slicing by: ['Restaurants']\n",
      "Samples in training set: 125311\n",
      "\n",
      "test MSE cultural 0.973345461503\n"
     ]
    }
   ],
   "source": [
    "x_train, y_train, x_test, y_test = training_test_split(x_cult, y_cult)\n",
    "print 'Slicing by:', SLICE_BY\n",
    "print 'Samples in slice:', len(x_cult)\n",
    "print\n",
    "\n",
    "reg = linear_model.Ridge(alpha=.5)\n",
    "reg.fit(x_cult, y_cult)\n",
    "\n",
    "print 'Intercept =', reg.intercept_\n",
    "print\n",
    "\n",
    "N = 20\n",
    "coefs = [(sub_ind_to_cult[i], c) for i, c in enumerate(reg.coef_)]\n",
    "for n, c in heapq.nlargest(N, coefs, key=lambda v: v[1]):\n",
    "  print n, c\n",
    "print\n",
    "    \n",
    "y_hat = reg.predict(x_cult)\n",
    "print \"MSE cultural\", np.sqrt(mean_squared_error(y_cult, y_hat))\n",
    "print\n",
    "\n",
    "x_train, y_train, x_test, y_test = training_test_split(x_cult, y_cult)\n",
    "print 'Slicing by:', SLICE_BY\n",
    "print 'Samples in training set:', len(x_train)\n",
    "print\n",
    "reg = linear_model.Ridge(alpha=0.7)\n",
    "reg.fit(x_train, y_train)\n",
    "y_hat = reg.predict(x_test)\n",
    "print \"test MSE cultural\", np.sqrt(mean_squared_error(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_rows_with(name_to_index, col, x, y):\n",
    "    ind = name_to_index[col]\n",
    "    print \"index from name \", ind\n",
    "    x_sub = []\n",
    "    y_sub = []\n",
    "    for i in range(len(x)):\n",
    "        if (x[i, ind] == 1):\n",
    "            x_sub.append(x[i])\n",
    "            y_sub.append(y[i])\n",
    "    return np.array(x_sub), np.array(y_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index from name  13\n",
      "Slicing by: ['Restaurants']\n",
      "Samples in training set: 375\n",
      "\n",
      "Intercept = 3.7904676204\n",
      "\n",
      "Ambience/hipster 0.605289672437\n",
      "/RestaurantsGoodForGroups 0.554916160494\n",
      "Vegan 0.403040577323\n",
      "Beer Bar 0.351060379974\n",
      "/RestaurantsPriceRange2/1 0.346367577859\n",
      "Food Stands 0.315131006487\n",
      "Tapas Bars 0.303033153525\n",
      "Ice Cream & Frozen Yogurt 0.302801784568\n",
      "Cocktail Bars 0.28816900886\n",
      "Dive Bars 0.286319047982\n",
      "/RestaurantsPriceRange2/2 0.283055523317\n",
      "/BYOBCorkage/no 0.277194944111\n",
      "Chicken Shop 0.271103139385\n",
      "Arts & Entertainment 0.271003048754\n",
      "Breweries 0.269218503615\n",
      "Food Delivery Services 0.268104876649\n",
      "/BikeParking 0.264128881308\n",
      "Food 0.25874456464\n",
      "/RestaurantsPriceRange2/3 0.253058114509\n",
      "French 0.251770540044\n",
      "\n",
      "...\n",
      "/BusinessAcceptsCreditCards -0.205559107343\n",
      "/RestaurantsAttire/casual -0.208075967246\n",
      "/Alcohol/full_bar -0.221551494099\n",
      "Burgers -0.223228439864\n",
      "/RestaurantsAttire/dressy -0.231310925496\n",
      "/RestaurantsTableService -0.233733126839\n",
      "Wine & Spirits -0.23453235213\n",
      "Beer -0.23453235213\n",
      "/OutdoorSeating -0.235850031893\n",
      "Imported Food -0.239193606074\n",
      "/Smoking/no -0.242115372631\n",
      "Pubs -0.274801624879\n",
      "Comfort Food -0.337164334186\n",
      "/DriveThru -0.341012645953\n",
      "/HappyHour -0.374252829173\n",
      "Diners -0.403767610044\n",
      "Japanese -0.4190061284\n",
      "Soup -0.488335096807\n",
      "Tex-Mex -0.493044724519\n",
      "Delis -0.703621807843\n",
      "\n",
      "test RMSE latin american 0.670314796243\n"
     ]
    }
   ],
   "source": [
    "x_latin, y_latin = get_rows_with(name_to_index, 'Latin American', x, y)\n",
    "x_train, y_train, x_test, y_test = training_test_split(x_latin, y_latin)\n",
    "print 'Slicing by:', SLICE_BY\n",
    "print 'Samples in training set:', len(x_train)\n",
    "print\n",
    "reg = linear_model.Ridge(alpha=1)\n",
    "reg.fit(x_train, y_train)\n",
    "\n",
    "print 'Intercept =', reg.intercept_\n",
    "print\n",
    "\n",
    "N = 20\n",
    "coefs = [(index_to_name[i], c) for i, c in enumerate(reg.coef_)]\n",
    "for n, c in heapq.nlargest(N, coefs, key=lambda v: v[1]):\n",
    "  print n, c\n",
    "print\n",
    "print '...'\n",
    "for n, c in reversed(heapq.nsmallest(N, coefs, key=lambda v: v[1])):\n",
    "  print n, c\n",
    "print\n",
    "\n",
    "    \n",
    "y_hat = reg.predict(x_test)\n",
    "print \"test RMSE latin american\", np.sqrt(mean_squared_error(y_test, y_hat))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index from name  1\n",
      "Slicing by: ['Restaurants']\n",
      "Samples in training set: 279\n",
      "\n",
      "Intercept = 3.49853853663\n",
      "\n",
      "Desserts 0.775382548588\n",
      "Shopping 0.516835207415\n",
      "Ice Cream & Frozen Yogurt 0.512691965255\n",
      "/OutdoorSeating 0.464077381114\n",
      "Food Trucks 0.389515046461\n",
      "Cocktail Bars 0.354636676856\n",
      "/Smoking/outdoor 0.346515666603\n",
      "Ambience/divey 0.324161571307\n",
      "Grocery 0.311283961649\n",
      "Caribbean 0.299661254319\n",
      "Caterers 0.272486351485\n",
      "/RestaurantsTakeOut 0.257592318417\n",
      "Breakfast & Brunch 0.244587318955\n",
      "Asian Fusion 0.235784639357\n",
      "Wine Bars 0.235100621206\n",
      "Nightlife 0.227976914596\n",
      "Vegan 0.227790810864\n",
      "Halal 0.202855195362\n",
      "/BusinessAcceptsBitcoin 0.199701996113\n",
      "/BikeParking 0.154012233499\n",
      "\n",
      "...\n",
      "Vegetarian -0.179681703066\n",
      "Buffets -0.180260284383\n",
      "Bakeries -0.186386235326\n",
      "Food Stands -0.19586311029\n",
      "Coffee & Tea -0.251799647995\n",
      "Thai -0.255390555707\n",
      "/DriveThru -0.261202726505\n",
      "/RestaurantsPriceRange2/2 -0.263833473511\n",
      "/RestaurantsPriceRange2/3 -0.275503291972\n",
      "Arts & Entertainment -0.288931108978\n",
      "/Smoking/no -0.292771457981\n",
      "Venues & Event Spaces -0.294148868574\n",
      "/GoodForKids -0.343740521782\n",
      "Bars -0.356303326625\n",
      "/RestaurantsPriceRange2/1 -0.36295522868\n",
      "American (Traditional) -0.370237618428\n",
      "Food Delivery Services -0.397074082582\n",
      "BusinessParking/validated -0.420367218516\n",
      "/NoiseLevel/very_loud -0.43944374033\n",
      "/HasTV -0.444010926649\n",
      "\n",
      "test RMSE pakistani 0.74083686966\n"
     ]
    }
   ],
   "source": [
    "x_part, y_part = get_rows_with(name_to_index, 'Pakistani', x, y)\n",
    "x_train, y_train, x_test, y_test = training_test_split(x_part, y_part)\n",
    "print 'Slicing by:', SLICE_BY\n",
    "print 'Samples in training set:', len(x_train)\n",
    "print\n",
    "reg = linear_model.Ridge(alpha=1)\n",
    "reg.fit(x_train, y_train)\n",
    "\n",
    "print 'Intercept =', reg.intercept_\n",
    "print\n",
    "\n",
    "N = 20\n",
    "coefs = [(index_to_name[i], c) for i, c in enumerate(reg.coef_)]\n",
    "for n, c in heapq.nlargest(N, coefs, key=lambda v: v[1]):\n",
    "  print n, c\n",
    "print\n",
    "print '...'\n",
    "for n, c in reversed(heapq.nsmallest(N, coefs, key=lambda v: v[1])):\n",
    "  print n, c\n",
    "print\n",
    "category_rmse = get_test_RMSE(x, y, get_categories)\n",
    "print \"RMSE for training on category only \", category_rmse\n",
    "    \n",
    "y_hat = reg.predict(x_test)\n",
    "print \"test RMSE pakistani\", np.sqrt(mean_squared_error(y_test, y_hat))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
