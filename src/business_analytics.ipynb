{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from collections import defaultdict\n",
    "import heapq\n",
    "import json\n",
    "import math\n",
    "import time\n",
    "\n",
    "import imageio\n",
    "import loading\n",
    "import tqdm\n",
    "\n",
    "from sklearn import linear_model\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"DEFINE CONSTANTS HERE\"\"\"\n",
    "\n",
    "DATA_PATH           = '../data'\n",
    "VISUALIZATION_PATH  = '../visualization'\n",
    "\n",
    "MISSING_VALUE       = '<NONE>'       # Used for the 'neighborhood' and 'city' attributes.\n",
    "DEFAULT_NEI_P       = 0.2            # Default percentile of neighborhoods to keep.\n",
    "DEFAULT_CITY_P      = 0.1            # Default percentile of cities to keep.\n",
    "DEFAULT_ATT_P       = 1.0            # Default percentile of attributes to keep.\n",
    "DEFAULT_CAT_P       = 0.5            # Default percentile of categories to keep.\n",
    "DEFAULT_HRS_P       = 1.0            # Default percentile of hours to keep.\n",
    "\n",
    "TIME_GRANULARITY    = 1              # Granularity (ticks/hr) of time calculations, factor of 60. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Loads the json file of the given dataset name.\"\"\"\n",
    "def load(name):\n",
    "  start = time.time()\n",
    "  data = loading.read_df_from_json('%s/%s.json' % (DATA_PATH, name))\n",
    "  print 'time to load \\'%s\\': %.3fs' % (name, time.time() - start)\n",
    "  return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Cleans the business dataset.\"\"\"\n",
    "def clean_business(business):\n",
    "  print 'Replacing %s with %s.' % (u'Montréal', u'Montreal')\n",
    "  business['city'].replace(to_replace=u'Montréal', \n",
    "                           value=u'Montreal',\n",
    "                           inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Plots all businesses on the world map for visualization purposes.\"\"\"\n",
    "def plot_business(business):\n",
    "  points = business[['latitude', 'longitude']]\n",
    "\n",
    "  img = imageio.imread(VISUALIZATION_PATH + '/raw_map.jpg').astype('int64')\n",
    "  img = img / 4               # Dim map.\n",
    "  img = img[8:-8,8:-8,:]      # Clip borders.\n",
    "  H, W, _ = img.shape\n",
    "  \n",
    "  scalar = 10                 # Amount to add to each channel.\n",
    "  delta = np.zeros((H, W), dtype='int64')\n",
    "  \n",
    "  def get_xy(latitude, longitude):\n",
    "    x = (W - 1) * (180.0 + longitude) / 360.0\n",
    "    y = (H - 1) * (90.0 - latitude) / 180.0\n",
    "    return int(x), int(y)\n",
    "\n",
    "  for row in points.itertuples():\n",
    "    latitude, longitude = row.latitude, row.longitude\n",
    "    if not math.isnan(latitude) and not math.isnan(longitude):\n",
    "      x, y = get_xy(latitude, longitude)\n",
    "      delta[y,x] += scalar\n",
    "\n",
    "  img += np.expand_dims(delta, axis=-1).repeat(3, axis=-1)\n",
    "  img = img.clip(0, 255).astype('uint8')\n",
    "  \n",
    "  return img"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Returns the count_dict as a sorted list.\"\"\"\n",
    "def to_list(count_dict):\n",
    "  return sorted([(k, count_dict[k]) for k in count_dict], key=lambda v: v[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Converts a dict of counts (key, int) into a list of top features.\n",
    "\n",
    "   Takes either top N (int) features, or top PERCENTILE (float) by occurrence.\n",
    "\n",
    "   Example usage:\n",
    "     top_features(count_dict, 0.1)                  # Returns top 10% of elements.\n",
    "     top_features(count_dict, top_n=5)              # Returns top 5 elements.\n",
    "\"\"\"\n",
    "def top_features(count_dict, percentile=None, n=None, verbose=True):\n",
    "  if n is None:\n",
    "    if percentile is None:\n",
    "      raise Exception\n",
    "    n = int(percentile * len(count_dict))\n",
    "\n",
    "  l = heapq.nlargest(n, count_dict, key=lambda k: count_dict[k])\n",
    "  \n",
    "  if verbose:\n",
    "    percentage = 0.0 if not len(count_dict) else 100.0 * n / len(count_dict)\n",
    "    params = (n, len(count_dict), percentage, 0 if not l else count_dict[l[-1]])\n",
    "    print 'Took %d elements out of %d (%2.1f%%). Cutoff was >= %d.' % params\n",
    "    \n",
    "  return l"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Methods for converting a tuple from itertuples() to a feature list.\"\"\"\n",
    "\n",
    "# Return neighborhood concatenated with city, or MISSING_VALUE if empty. \n",
    "def get_neighborhood(tup):\n",
    "  assert type(tup.neighborhood) is unicode\n",
    "  return [tup.neighborhood + '/' + tup.city if tup.neighborhood else MISSING_VALUE]\n",
    "\n",
    "# Return city, or MISSING_VALUE if empty.\n",
    "def get_city(tup):\n",
    "  assert type(tup.city) is unicode\n",
    "  return [tup.city if tup.city else MISSING_VALUE]\n",
    "\n",
    "# Recursively process attributes dict to get indicators for all attributes.\n",
    "def get_attributes(tup):\n",
    "  def _recurse(attributes, prefix):\n",
    "    assert type(attributes) is dict\n",
    "    l = []\n",
    "    for k, v in attributes.items():\n",
    "      if type(v) is bool:\n",
    "        l.append(prefix + '/' + k)\n",
    "      elif type(v) is unicode:\n",
    "        l.append(prefix + '/' + k + '/' + v)\n",
    "      elif type(v) is int:\n",
    "        l.append(prefix + '/' + k + '/' + str(v))\n",
    "      elif type(v) is dict:\n",
    "        l += _recurse(attributes[k], prefix=k)\n",
    "      else:\n",
    "        assert False  # Invalid type in attributes.\n",
    "    return l\n",
    "  return _recurse(tup.attributes, prefix='')\n",
    "\n",
    "# Return categories.\n",
    "def get_categories(tup):\n",
    "  assert type(tup.categories) is list\n",
    "  return tup.categories\n",
    "\n",
    "\"\"\"Helper methods for get_hours(), which determines which ticks of time the business is open.\n",
    "   Each tick of time corresponds to an index in [0, _max_ticks()).\n",
    "\n",
    "   In _time_to_dt_index, ROUND_UP determines what happens when the time falls\n",
    "   in between ticks. By default, the time is rounded up.\n",
    "\"\"\"\n",
    "# An ordering of the days of the week, and a map from str --> index\n",
    "_day_order = ['Sunday', 'Monday', 'Tuesday', 'Wednesday', 'Thursday', 'Friday', 'Saturday']\n",
    "_day_index = {d:i for i, d in enumerate(_day_order)}\n",
    "\n",
    "# Maximum number of ticks\n",
    "def _max_ticks():\n",
    "  return 7 * 24 * TIME_GRANULARITY\n",
    "\n",
    "# Given day of the week and time, returns the corresponding tick in [0, _max_ticks()).\n",
    "def _time_to_dt_index(day, time, round_up=True):\n",
    "  [hour, minutes] = time.split(':')\n",
    "  hour_index = int(hour) * TIME_GRANULARITY\n",
    "  min_index  = int(minutes) / (60 / TIME_GRANULARITY)\n",
    "  if round_up and int(minutes) % (60 / TIME_GRANULARITY) > 0:\n",
    "    min_index += 1\n",
    "  return _day_index[day] * 24 * TIME_GRANULARITY + hour_index + min_index\n",
    "\n",
    "# Cache the string corresponding to each tick of time.\n",
    "_timestr_cache = ['%s/%02d:%02d' % (_day_order[day], hour, min_index * 60 / TIME_GRANULARITY)\n",
    "                  for day in range(0, 7)\n",
    "                  for hour in range(0, 24)\n",
    "                  for min_index in range(0, TIME_GRANULARITY)]\n",
    "\n",
    "def get_hours(tup):\n",
    "  assert type(tup.hours) is dict\n",
    "  l = []\n",
    "  for day, hours in tup.hours.items():\n",
    "    open_time, close_time = hours.split('-')\n",
    "    open_index = _time_to_dt_index(day, open_time)\n",
    "    close_index = _time_to_dt_index(day, close_time)\n",
    "\n",
    "    assert 0 <= open_index and close_index <= _max_ticks()\n",
    "    \n",
    "    # Handle the case where close_index is for the following day.\n",
    "    if close_index <= open_index:\n",
    "      close_index += 24 * TIME_GRANULARITY\n",
    "    \n",
    "    # Append the slice of the _time_str_cache, handling wrap-around appropriately.\n",
    "    l += _timestr_cache[open_index:min(close_index, _max_ticks())]\n",
    "    if close_index > _max_ticks():\n",
    "      l += _timestr_cache[0:close_index - _max_ticks()]\n",
    "      \n",
    "  return l\n",
    "\n",
    "# Function to retrieve all features of a given row tuple.\n",
    "all_fns = [get_neighborhood, get_city, get_attributes, get_categories, get_hours]\n",
    "def get_all_features(tup):\n",
    "  all_features = set([])\n",
    "  for fn in all_fns:\n",
    "    all_features |= set(fn(tup))\n",
    "  return all_features\n",
    "\n",
    "# Returns the value to regress on for the row tuple.\n",
    "def get_target(tup):\n",
    "  return float(tup.stars)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Get the features that we will use for \n",
    "   neighborhoods, cities, attributes, categories.\n",
    "\n",
    "   By default, take:\n",
    "     TOP 20% OF neighborhoods\n",
    "     TOP 10% OF cities\n",
    "         ALL OF attributes\n",
    "     TOP 50% OF categories\n",
    "         ALL OF hours\n",
    "   \n",
    "   Returns a length 6 tuple:\n",
    "     (nei_set, city_set, att_set, cat_set, hours_set, debug_vals)\n",
    "\"\"\"\n",
    "def get_feature_sets(business, \n",
    "                     percentiles=[DEFAULT_NEI_P, DEFAULT_CITY_P, DEFAULT_ATT_P, \n",
    "                                  DEFAULT_CAT_P, DEFAULT_HRS_P]):\n",
    "  \n",
    "  fn_counts_percentile = zip(all_fns, [defaultdict(int) for _ in range(len(all_fns))], percentiles)\n",
    "\n",
    "  for tup in tqdm.tqdm(business.itertuples()):\n",
    "    for fn, counts, _ in fn_counts_percentile:\n",
    "      features = fn(tup)\n",
    "      for f in features:\n",
    "        counts[f] += 1\n",
    "\n",
    "  debug_val = [(fn.__name__, to_list(counts)) for fn, counts, _ in fn_counts_percentile]\n",
    "  \n",
    "  return [top_features(counts, percentile) for fn, counts, percentile in fn_counts_percentile], debug_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Gets a mapping from feature name to feature index and vice versa.\"\"\"\n",
    "def get_feature_maps(feature_sets, start_index=0):  \n",
    "  # Assert that there are no overlapping names.\n",
    "  union = set([])\n",
    "  for s in feature_sets:\n",
    "    union |= set(s)\n",
    "  assert len(union) == sum([len(s) for s in feature_sets])\n",
    "\n",
    "  name_to_index, index_to_name = {}, {}\n",
    "  for i, feature in enumerate(union):\n",
    "    name_to_index[feature] = start_index + i\n",
    "    index_to_name[start_index + i] = feature\n",
    "\n",
    "  return name_to_index, index_to_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Creates a list of data points for multivariate linear regression.\"\"\"\n",
    "def get_training_data(business, name_to_index):\n",
    "  x = np.zeros((business.shape[0], len(name_to_index)), dtype='float32')\n",
    "  y = np.zeros(business.shape[0], dtype='float32')\n",
    "  \n",
    "  for i, tup in tqdm.tqdm(enumerate(business.itertuples())):\n",
    "    all_features = get_all_features(tup)\n",
    "    for f in all_features:\n",
    "      if f in name_to_index:\n",
    "        x[i,name_to_index[f]] = 1\n",
    "    y[i] = get_target(tup)\n",
    "  \n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run business regression.\n",
    "\n",
    "NOT USABLE:\n",
    "- business_id\n",
    "- name\n",
    "\n",
    "PROBABLY NOT USABLE:\n",
    "- latitude\n",
    "- longitude\n",
    "- postal code\n",
    "- address\n",
    "- is_open\n",
    "- review_count\n",
    "- state\n",
    "\n",
    "REGRESS ON:\n",
    "- stars  (1, 1.5, 2, 2.5, 3, 3.5, 4, 4.5, 5)\n",
    "\n",
    "FEATURES:\n",
    "- neighborhood/city  --> indicators (~20%)\n",
    "- city               --> indicators (~10%)\n",
    "- attributes         --> indicators (process types differently, each has a separate indicator)\n",
    "- categories         --> indicators (~50%)\n",
    "- hours              --> indicators for every hour/half hour/quarter of the hour"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "time to load 'business': 18.691s\n",
      "Replacing Montréal with Montreal.\n"
     ]
    }
   ],
   "source": [
    "business = load('business')\n",
    "clean_business(business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "imageio.freeimage warning: Could not determine tag type of u'XMLPacket'.\n"
     ]
    }
   ],
   "source": [
    "img = plot_business(business)\n",
    "plt.imshow(img)\n",
    "plt.show()\n",
    "imageio.imsave(VISUALIZATION_PATH + '/business_map.jpg', img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Took 146 elements out of 730 (20.0%). Cutoff was >= 77.\n",
      "Took 100 elements out of 1009 (9.9%). Cutoff was >= 112.\n",
      "Took 100 elements out of 100 (100.0%). Cutoff was >= 35.\n",
      "Took 620 elements out of 1240 (50.0%). Cutoff was >= 58.\n",
      "Took 168 elements out of 168 (100.0%). Cutoff was >= 6635.\n"
     ]
    }
   ],
   "source": [
    "feature_sets, debug_val = get_feature_sets(business)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "feature_maps = get_feature_maps(feature_sets)\n",
    "name_to_index, index_to_name = feature_maps  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                          \r"
     ]
    }
   ],
   "source": [
    "x, y = get_training_data(business, name_to_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LinearRegression(copy_X=True, fit_intercept=True, n_jobs=1, normalize=False)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reg = linear_model.LinearRegression()\n",
    "reg.fit(x, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Intercept = 3.49948661548\n",
      "\n",
      "GoodForMeal/dessert 520.430725098\n",
      "Ambience/trendy 358.684173584\n",
      "Ambience/casual 352.772460938\n",
      "Ambience/romantic 352.304962158\n",
      "Ambience/classy 352.304779053\n",
      "Ambience/intimate 351.43548584\n",
      "BestNights/saturday 307.764770508\n",
      "HairSpecializesIn/africanamerican 7.45800495148\n",
      "BusinessParking/garage 4.006295681\n",
      "Mags 2.96031188965\n",
      "DietaryRestrictions/vegetarian 2.82560563087\n",
      "HairSpecializesIn/extensions 2.65782833099\n",
      "Music/no_music 2.57879567146\n",
      "Music/karaoke 2.29759383202\n",
      "DietaryRestrictions/soy-free 1.96690750122\n",
      "...\n",
      "HairSpecializesIn/asian -4.60122871399\n",
      "Music/background_music -5.20290708542\n",
      "GoodForMeal/lunch -8.15113449097\n",
      "GoodForMeal/breakfast -10.1808385849\n",
      "GoodForMeal/brunch -10.180847168\n",
      "GoodForMeal/latenight -10.1809062958\n",
      "BestNights/sunday -47.3309783936\n",
      "BestNights/thursday -47.6847076416\n",
      "BestNights/wednesday -49.9632453918\n",
      "BestNights/friday -51.8051681519\n",
      "BestNights/monday -54.8503570557\n",
      "BestNights/tuesday -56.0668830872\n",
      "Ambience/touristy -400.365783691\n",
      "GoodForMeal/dinner -481.648925781\n",
      "Ambience/upscale -1366.99560547\n"
     ]
    }
   ],
   "source": [
    "print 'Intercept =', reg.intercept_\n",
    "print\n",
    "\n",
    "N = 15\n",
    "coefs = [(index_to_name[i], c) for i, c in enumerate(reg.coef_)]\n",
    "for n, c in heapq.nlargest(N, coefs, key=lambda v: v[1]):\n",
    "  print n, c\n",
    "print '...'\n",
    "for n, c in reversed(heapq.nsmallest(N, coefs, key=lambda v: v[1])):\n",
    "  print n, c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Splits list of data points 80/20 for training versus test.\"\"\"\n",
    "def training_test_split(x, y):\n",
    "    indices = np.random.permutation(x.shape[0])\n",
    "    cutoff = int(.8 * x.shape[0])\n",
    "    training_idx, test_idx = indices[:cutoff], indices[cutoff+1:]\n",
    "    x_train, x_test = x[training_idx,:], x[test_idx,:]\n",
    "    y_train, y_test = y[training_idx], y[test_idx]\n",
    "    return x_train, y_train, x_test, y_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train, x_test, y_test = training_test_split(x, y)\n",
    "reg.fit(x_train, y_train)\n",
    "y_pred = reg.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE For Test Set  0.718638640628\n"
     ]
    }
   ],
   "source": [
    "rmse = mean_squared_error(y_test, y_pred)\n",
    "print \"RMSE For Test Set \", rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Function to retrieve all non-category features of a given row tuple.\n",
    "def get_non_categories(tup):\n",
    "  non_categories = set([])\n",
    "  for fn in [get_neighborhood, get_city, get_attributes, get_hours]:\n",
    "    non_categories |= set(fn(tup))\n",
    "  return non_categories\n",
    "\n",
    "# Function to retrieve all location features of a given row tuple.\n",
    "def get_locations(tup):\n",
    "  locations = set([])\n",
    "  for fn in [get_neighborhood, get_city]:\n",
    "    locations |= set(fn(tup))\n",
    "  return locations\n",
    "\n",
    "\"\"\"Creates a list of data points for multivariate linear regression with category data only.\"\"\"\n",
    "def get_subset_training_data(business, name_to_index, retrieving_function):\n",
    "  x = np.zeros((business.shape[0], len(name_to_index)), dtype='float32')\n",
    "  y = np.zeros(business.shape[0], dtype='float32')\n",
    "  \n",
    "  for i, tup in tqdm.tqdm(enumerate(business.itertuples())):\n",
    "    all_features = retrieving_function(tup)\n",
    "    for f in all_features:\n",
    "      if f in name_to_index:\n",
    "        x[i,name_to_index[f]] = 1\n",
    "    y[i] = get_target(tup)\n",
    "  \n",
    "  return x, y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\" Find test RMSE for subset of training inputs. \n",
    "    x, y, are data to be split into training and test.\n",
    "    retrieving_function specified subset of inputs\n",
    "\"\"\"\n",
    "def get_test_RMSE(x, y, retrieving_function):\n",
    "    x, y = get_subset_training_data(business, name_to_index, retrieving_function)\n",
    "    x_train, y_train, x_test, y_test = training_test_split(x, y)\n",
    "    reg.fit(x_train, y_train)\n",
    "    y_pred = reg.predict(x_test)\n",
    "    return mean_squared_error(y_test, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training on category only  0.879065322512\n"
     ]
    }
   ],
   "source": [
    "category_rmse = get_test_RMSE(x, y, get_categories)\n",
    "print \"RMSE for training on category only \", category_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training on non-categories only  0.83892275188\n"
     ]
    }
   ],
   "source": [
    "noncat_rmse = get_test_RMSE(x, y, get_non_categories)\n",
    "print \"RMSE for training on non-categories only \", noncat_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training on neighborhood and city only  0.934115905807\n"
     ]
    }
   ],
   "source": [
    "location_rmse = get_test_RMSE(x, y, get_locations)\n",
    "print \"RMSE for training on neighborhood and city only \", location_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training on hours only  0.900267663514\n"
     ]
    }
   ],
   "source": [
    "hours_rmse = get_test_RMSE(x, y, get_hours)\n",
    "print \"RMSE for training on hours only \", hours_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE for training on attributes only  0.882129318197\n"
     ]
    }
   ],
   "source": [
    "attributes_rmse = get_test_RMSE(x, y, get_attributes)\n",
    "print \"RMSE for training on attributes only \", attributes_rmse"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
